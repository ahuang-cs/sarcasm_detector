{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prerequisites\n",
    "- Python 3\n",
    "- Python jupyter module\n",
    "- Python clearml module\n",
    "- [Cuda 12.3](https://developer.nvidia.com/cuda-downloads)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set ENV vars"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=http://clearml.ad.ai.sdl.com:8080\n",
      "env: CLEARML_API_HOST=http://clearml.ad.ai.sdl.com:8008\n",
      "env: CLEARML_FILES_HOST=http://clearml.ad.ai.sdl.com:8081\n",
      "env: CLEARML_API_ACCESS_KEY=SKXA1SPMCLDW7NH3JG5F\n",
      "env: CLEARML_API_SECRET_KEY=bMKqfrJnZOWTIJEgnYVfBSncCub4XBPxGYhspwitWFmREfYKUe\n"
     ]
    }
   ],
   "source": [
    "%env CLEARML_WEB_HOST=http://clearml.ad.ai.sdl.com:8080\n",
    "%env CLEARML_API_HOST=http://clearml.ad.ai.sdl.com:8008\n",
    "%env CLEARML_FILES_HOST=http://clearml.ad.ai.sdl.com:8081\n",
    "%env CLEARML_API_ACCESS_KEY=SKXA1SPMCLDW7NH3JG5F\n",
    "%env CLEARML_API_SECRET_KEY=bMKqfrJnZOWTIJEgnYVfBSncCub4XBPxGYhspwitWFmREfYKUe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: http://clearml.ad.ai.sdl.com:8080/projects/8d45228c5d8e47c0be6c81baf44c0eea/experiments/fd9a1a524e4149ecbd29cae28c7888f6/output/log\n",
      "ClearML dataset page: http://clearml.ad.ai.sdl.com:8080/datasets/simple/8d45228c5d8e47c0be6c81baf44c0eea/experiments/fd9a1a524e4149ecbd29cae28c7888f6\n"
     ]
    }
   ],
   "source": [
    "from clearml import Dataset\n",
    "dataset = Dataset.create(\n",
    "  dataset_name='kaggle_sarcasm',\n",
    "  dataset_project='sarcasm_detector',\n",
    "  dataset_version=\"1.0\",\n",
    "  description='From https://www.kaggle.com/datasets/danofer/sarcasm'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Upload files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SHA2 hash for 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash generation completed\n",
      "Uploading dataset changes (2 files compressed to 28.69 MiB) to http://clearml.ad.ai.sdl.com:8081\n",
      "File compression and upload completed: total size 28.69 MiB, 1 chunk(s) stored (average size 28.69 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.add_files(\"data\")\n",
    "dataset.upload()\n",
    "dataset.finalize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start sklearn training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying (Retry(total=239, connect=239, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D3D269BD90>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /auth.login\n",
      "Retrying (Retry(total=238, connect=238, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D3D32F1410>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /auth.login\n",
      "Retrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D3D3253210>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /auth.login\n",
      "Retrying (Retry(total=236, connect=236, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D3D32C6610>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /auth.login\n"
     ]
    }
   ],
   "source": [
    "from train_sklearn import SklearnTrainer\n",
    "\n",
    "sarcasm_trainer = SklearnTrainer(subset_size=1000)\n",
    "sarcasm_trainer.train()\n",
    "sarcasm_trainer.task.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check pytorch versions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cpu\n",
      "**********\n",
      "_CUDA version: \n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Sep__8_19:56:38_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.52\n",
      "Build cuda_12.3.r12.3/compiler.33281558_0\n",
      "**********\n",
      "CUDNN version: None\n",
      "Available GPU devices: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDNN version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mcudnn\u001B[38;5;241m.\u001B[39mversion()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAvailable GPU devices: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice_count()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDevice Name: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_device_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\sarcasm_detector\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:419\u001B[0m, in \u001B[0;36mget_device_name\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_device_name\u001B[39m(device: Optional[_device_t] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    408\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Gets the name of a device.\u001B[39;00m\n\u001B[0;32m    409\u001B[0m \n\u001B[0;32m    410\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;124;03m        str: the name of the device\u001B[39;00m\n\u001B[0;32m    418\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_device_properties\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mname\n",
      "File \u001B[1;32m~\\PycharmProjects\\sarcasm_detector\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:449\u001B[0m, in \u001B[0;36mget_device_properties\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_device_properties\u001B[39m(device: _device_t) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _CudaDeviceProperties:\n\u001B[0;32m    440\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Gets the properties of a device.\u001B[39;00m\n\u001B[0;32m    441\u001B[0m \n\u001B[0;32m    442\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    447\u001B[0m \u001B[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001B[39;00m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 449\u001B[0m     \u001B[43m_lazy_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# will define _get_device_properties\u001B[39;00m\n\u001B[0;32m    450\u001B[0m     device \u001B[38;5;241m=\u001B[39m _get_device_index(device, optional\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m device \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m device_count():\n",
      "File \u001B[1;32m~\\PycharmProjects\\sarcasm_detector\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m     )\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    293\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print('*'*10)\n",
    "print(f'_CUDA version: ')\n",
    "!nvcc --version\n",
    "print('*'*10)\n",
    "print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "print(f'Device Name: {torch.cuda.get_device_name()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=7dcc42c34c7a4b8ca40d501d65c03668\n",
      "ClearML results page: http://clearml.ad.ai.sdl.com:8080/projects/2bb1d55901b4402d80b7120beadfc8dc/experiments/7dcc42c34c7a4b8ca40d501d65c03668/output/log\n",
      "2023-10-24 21:47:24,649 - clearml.Task - INFO - Storing jupyter notebook directly as code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default-4fa3bd361ffaa254\n",
      "Found cached dataset csv (file://C:/Users/Andrew/.cache/huggingface/datasets/csv/default-4fa3bd361ffaa254/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m Task\u001B[38;5;241m.\u001B[39minit(project_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msarcasm_detector\u001B[39m\u001B[38;5;124m\"\u001B[39m, task_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistilBert Training\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m sarcasm_trainer \u001B[38;5;241m=\u001B[39m SarcasmTrainer(subset_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43msarcasm_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m Task\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\PycharmProjects\\sarcasm_detector\\train_transformer.py:100\u001B[0m, in \u001B[0;36mSarcasmTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 100\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    101\u001B[0m     data_collator, tokenized_dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenize_data(dataset)\n\u001B[0;32m    103\u001B[0m     training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m    104\u001B[0m         output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmy_awesome_model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    105\u001B[0m         learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    113\u001B[0m         load_best_model_at_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    114\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\sarcasm_detector\\train_transformer.py:68\u001B[0m, in \u001B[0;36mSarcasmTrainer.get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     62\u001B[0m     local_dataset_path \u001B[38;5;241m=\u001B[39m Path(Dataset\u001B[38;5;241m.\u001B[39mget(\n\u001B[0;32m     63\u001B[0m         dataset_project\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msarcasm_detector\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     64\u001B[0m         dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkaggle_sarcasm\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     65\u001B[0m         alias\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkaggle_sarcasm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     66\u001B[0m     )\u001B[38;5;241m.\u001B[39mget_local_copy())\n\u001B[1;32m---> 68\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlocal_dataset_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_dataset_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mall\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mtrain_test_split(\n\u001B[0;32m     74\u001B[0m         test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n\u001B[0;32m     75\u001B[0m         shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     76\u001B[0m         seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseed\n\u001B[0;32m     77\u001B[0m     )\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubset_size:\n",
      "File \u001B[1;32m~\\PycharmProjects\\sarcasm_detector\\venv\\Lib\\site-packages\\datasets\\load.py:1769\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001B[0m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1767\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[0;32m   1768\u001B[0m )\n\u001B[1;32m-> 1769\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_verifications\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_verifications\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1770\u001B[0m \u001B[38;5;66;03m# Rename and cast features to match task schema\u001B[39;00m\n\u001B[0;32m   1771\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m task \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\sarcasm_detector\\venv\\Lib\\site-packages\\datasets\\builder.py:1051\u001B[0m, in \u001B[0;36mDatasetBuilder.as_dataset\u001B[1;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001B[0m\n\u001B[0;32m   1049\u001B[0m is_local \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m is_remote_filesystem(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fs)\n\u001B[0;32m   1050\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local:\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading a dataset cached in a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fs)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_dir):\n\u001B[0;32m   1053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[0;32m   1054\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: could not find data in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please make sure to call \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1055\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilder.download_and_prepare(), or use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1056\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasets.load_dataset() before trying to access the Dataset object.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1057\u001B[0m     )\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "from clearml import Task\n",
    "from train_transformer import SarcasmTrainer\n",
    "\n",
    "Task.add_requirements(\"torch\")\n",
    "Task.init(project_name=\"sarcasm_detector\", task_name=\"DistilBert Training\")\n",
    "sarcasm_trainer = SarcasmTrainer(subset_size=1000)\n",
    "sarcasm_trainer.train()\n",
    "Task.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
